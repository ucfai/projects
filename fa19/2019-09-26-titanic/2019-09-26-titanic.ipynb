{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nb-title": true,
    "title": "Understanding the Titanic Disaster"
   },
   "source": [
    "<img src=\"https://ucfai.org/data-science/fa19/2019-09-26-titanic/titanic/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Understanding the Titanic Disaster </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> Alec Kerrigan</strong>\n",
    "        (<a href=\"https://github.com/ahkerrigan\">@ahkerrigan</a>)\n",
    "     on 2019-09-26</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI@UCF Data Science Group Fall 2019 Titanic Workshop Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/titanic/train.csv\")\n",
    "test = pd.read_csv(\"../input/titanic/test.csv\")\n",
    "IDtest = test[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to make changes to all data, we need to combine train and test for the time being\n",
    "train_len = len(train)\n",
    "dataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it comes to cleaning data, what is the very first thing you should do?\n",
    "# Always manage your missing values. \n",
    "\n",
    "# Fill empty and NaNs values with NaN\n",
    "dataset = dataset.fillna(np.nan)\n",
    "\n",
    "# Check for Null values\n",
    "dataset.isnull().sum()\n",
    "\n",
    "# The reason why there is a significant number of null values in survived is only because we combiend the train and the test, and obviously we don't have the solutions for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to check what just the train looks like, we still have it in memory\n",
    "\n",
    "train.info()\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \n",
    "g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a heatmap, we can check to see how our numerical values correlate with each other. As you can see, the only numerical value that seemst o really correlate with \n",
    "# survival is Fare. That's not to say that the others are useless, but intuitively you can imagine bigger fair = rich = surivived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What definately is possible is that the other features have subpopulations that have actual correlation. That is to say``````````````````````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore SibSp feature vs Survived\n",
    "g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \n",
    "palette = \"muted\")\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does thisd tell us about number of siblings?\n",
    "# It tells us that while 0-2 siblings have a fairly average chance of survival, 3-4 have a dramatically smaller chance.\n",
    "# This means that while sibling count isn't good by itself, knowing whether they have 3-4 is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Parch feature vs Survived\n",
    "g  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \n",
    "palette = \"muted\")\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What this tells us is that smaller families (1-2) have better chance of survival than people without families, or people wtih large families\n",
    "# Why am I not saying that families of 3 have the best chance of surivial?\n",
    "# While sure, the average seems marginally higher, the uncertainty is much greater around families of 3, meaning the values are more spread out.\n",
    "# Families of 1-2 are much more certain to have more than 50% chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets take a look at a Facet graph of Age\n",
    "# Explore Age vs Survived\n",
    "g = sns.FacetGrid(train, col='Survived')\n",
    "g = g.map(sns.distplot, \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this tell us\n",
    "# Before in our heatmap, we noted that Age does not seem correlated with survival, but this graph tells a different story. \n",
    "# The distribution seems somewhat normal, except for the unusual jump in survivability of infants\n",
    "# This means it might be valauble to classify passenger into age groups, rather than leave their age\n",
    "# let's try superimposing these to get a clearer picture\n",
    "\n",
    "# Explore Age distibution \n",
    "g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\n",
    "g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g = g.legend([\"Not Survived\",\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's move onto Fare\n",
    "dataset[\"Fare\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theres only a single missing value of fare, what should we do with it?\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Fare distribution \n",
    "g = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\n",
    "g = g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oof, this is extremly skewed. This means that our model is going to massively overweight values on the right end. Therefore, we should probobly transform it into a log function\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\n",
    "g = g.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That looks much, much better right?\n",
    "# Let's move onto sex\n",
    "g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\n",
    "g = g.set_ylabel(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh wow, men only have about a 20% chance of survival, while women have 70%\n",
    "# You could actually stop everything right now and just predict survival based on gender and get ~75% accuracy wiht these numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Pclass vs Survived\n",
    "g = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \n",
    "palette = \"muted\")\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you would expect, chance of survival is pretty much directly correlated with class\n",
    "# Explore Pclass vs Survived by Sex\n",
    "g = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train,\n",
    "                   size=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we saw earlier with our exploration of Age, there is clearly some information hidden within\n",
    "# However there are 300 values missing. This is way too many to simply replace with the mean (because 300 missing is enough to shift the mean to a false value)\n",
    "# The solution: Let's see if there is some other variable that correlates with Age, and use that to predict what the missing age might be\n",
    "# Explore Age vs Sex, Parch , Pclass and SibSP\n",
    "g = sns.factorplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\n",
    "g = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\n",
    "g = sns.factorplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\n",
    "g = sns.factorplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What can we conclude from these graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of age seems to be the same across male and femlae,  the higher the class of passenger, the older they are\n",
    "# Parch (parents/children) seem to be postively correlated wtih age, while sibling count engativly correlated. T\n",
    "# That is to say, older passengers tend to have more children/parents, while younger passengers have more siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and convert sex to a numerical value since we missed it\n",
    "dataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look directly at the correlation between numerical features\n",
    "g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a good example of our visual intuition being wrong. We were correct that Age and Sex have nothing to do with each other\n",
    "# But parent/childen is actually negativly correlated with age, as is class and sibling count\n",
    "# Heres how we're going to do it. For every missing Age value, we are going to find rows with the same Sibling, parch, and class values as that row with missing age, and average those ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing value of Age \n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how things changed\n",
    "g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")\n",
    "g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"violin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So theres still not correlation between ages and survival, except for that little blip at the buttom of the survived violen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's time for the best part: Feature engineering\n",
    "# Quesion: Should we keep the name of passengers? How could the name be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Title from Name\n",
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
    "dataset[\"Title\"] = pd.Series(dataset_title)\n",
    "dataset[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(x=\"Title\",data=dataset)\n",
    "g = plt.setp(g.get_xticklabels(), rotation=45) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So it would be a waste of time to worry about titles that only appear once or twice, so let's just change them to \"rare\"\n",
    "# Then, let's map each title to \n",
    "# Convert to categorical values for title Title \n",
    "dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "dataset[\"Title\"] = dataset[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.countplot(dataset[\"Title\"])\n",
    "g = g.set_xticklabels([\"Master\",\"Miss/Ms/Mme/Mlle/Mrs\",\"Mr\",\"Rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\n",
    "g = g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, we already know how important sex was to survival, but now \"master\" is revealing the importance of children as well\n",
    "# As we know \"Women and children first\"\n",
    "# Drop Name variable\n",
    "dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\n",
    "\n",
    "# If you recall, the only useful information from Age was that children had a slight advantage from\n",
    "# survival. However, most Ages were actually missing. There were no missing titles, so we're instead\n",
    "# going to rely on the existing of the \"master\" title to figure out who is a child.\n",
    "# Did we waste a ton of time cleaning up the Ages? Not really, a lot of the time when working on data\n",
    "# you find that previous approaches were not as good as you originally expected\n",
    "dataset.drop(labels = [\"Age\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go over to family size again, since we talked about that a lot\n",
    "# Let's examine size of family, including the passenger themselves\n",
    "# Create a family size descriptor from SibSp and Parch\n",
    "dataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)\n",
    "g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets break these into 4 categories\n",
    "# Create new feature of family size\n",
    "dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've essentially turned family size into a binary value, since theres no clear smooth correlation`\n",
    "g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "g = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gong to convert Title and embarked to binary values (a different column for each possible title/embark point)\n",
    "# dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about cabin?\n",
    "dataset[\"Cabin\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an old version of this, we tried this with Cabin, and in fact using it drops our score my 2 percent. Try running it and see what happens\n",
    "# Can you explain why this is actually suboptimal?\n",
    "\"\"\"\n",
    "\n",
    "# That is...a lot of missing values```````````````````````````````````````````````\n",
    "# Let's just replace missing values with an X, indicating no cabin listed\n",
    "dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\n",
    "#g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket may have some information, buts its very likely to be similar to fair, so lets just drop it. Let's drop passengerID as well\n",
    "dataset.drop(labels = [\"PassengerId\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, time for some machine learning!\n",
    "## Separate train dataset and test dataset\n",
    "\n",
    "train = dataset[:train_len]\n",
    "test = dataset[train_len:]\n",
    "test.drop(labels=[\"Survived\"],axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate train features and label \n",
    "\n",
    "train[\"Survived\"] = train[\"Survived\"].astype(int)\n",
    "\n",
    "Y_train = train[\"Survived\"]\n",
    "\n",
    "X_train = train.drop(labels = [\"Survived\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a cross-validator for our hyperparameter searcher\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,Y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVC classifier\n",
    "SVMC = SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsSVMC.fit(X_train,Y_train)\n",
    "\n",
    "SVMC_best = gsSVMC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting tunning\n",
    "\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train,Y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train,Y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how they all do\n",
    "# Note: I chose these 4 classifiers for you because I already know they do well for this problem\n",
    "# As an excercise, look into more than 10 classifiers and look up how to rank their cross-validation score\n",
    "test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\n",
    "test_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
    "test_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\n",
    "test_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n",
    "\n",
    "\n",
    "# Concatenate all classifier results\n",
    "ensemble_results = pd.concat([test_Survived_RFC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n",
    "\n",
    "\n",
    "g= sns.heatmap(ensemble_results.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final step: We will have these 4 models vote on each possible prediction\n",
    "votingC = VotingClassifier(estimators=[('rfc', RFC_best),\n",
    "('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "\n",
    "votingC = votingC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's send our submission to a CSV\n",
    "test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n",
    "\n",
    "results = pd.concat([IDtest,test_Survived],axis=1)\n",
    "\n",
    "results.to_csv(\"TitanicSubmission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "authors": [
    {
     "author": "Alec Kerrigan",
     "github": "ahkerrigan",
     "web": null
    }
   ],
   "categories": [
    "fa19"
   ],
   "date": "2019-09-26",
   "description": "The titanic dataset is often considered the \"hello world\" of data science. In this workshop, we will use this relatively simple dataset to help get you comfortable with the kind of workflow that will be used in the data science group. By the end of this workshop, all groups will be comfortable with the end-to-end process of data analysis, model selection, model fitting, and finally submission to kaggle. As always, you will have the opporunity to compete with the other groups!",
   "tags": [
    "machine learning",
    "data science",
    "titanic",
    "statistics"
   ],
   "title": "Understanding the Titanic Disaster"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
